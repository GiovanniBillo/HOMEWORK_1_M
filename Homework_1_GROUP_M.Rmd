
---
title: "Homework 1"
author: "Group M: Billo, Pizzignacco, El Gataa,  Shahzad"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
# Load necessary libraries here if any
#### TO USE GITHUB ####
install.packages("usethis")
library(usethis)

#### other libraries ####
library(dplyr)
```

## Introduction

*Briefly describe the purpose of the homework or tasks to be accomplished.*

## Block B

### CS: Chapter 1, exercises 1.1, 1.6

## Exercise 1: Deriving the CDF and Quantile Function 
Given \[ x \geq 0 \] and \[
f(x) = \lambda e^{-\lambda x}
\]
the CDF can be derived:
\[
F(x) = \int_{0}^{x} f(t) \, dt 
     = \int_{0}^{x} \lambda e^{-\lambda t} \, dt 
     = \lambda[\frac{-e^{-\lambda x}}{\lambda} + \frac{1}{\lambda} ]
\]
\[F(x) = 1 - e^{-\lambda x}\]

Now having the cumulative distribution function (CDF) \( F(x) = 1 - e^{-\lambda x} \), we want to find the quantile function, i.e., its inverse.

1. Starting with:
   \[
   y = 1 - e^{-\lambda x}
   \]

2. Rearrange to isolate \( x \):
   \[
   e^{-\lambda x} = 1 - y
   \]
   \[
   -\lambda x = \ln(1 - y)
   \]
   \[
   x = -\frac{\ln(1 - y)}{\lambda}
   \]

Thus, the quantile function is:
\[
x = -\frac{\ln(1 - y)}{\lambda}
\]

## Exercise 2.1: Finding \( P(X \leq \lambda) \)

\( F(\lambda) = P(X \leq \lambda)\)
\( F(\lambda) = 1 - e^{-\lambda \lambda}\)
\( F(\lambda) = 1 - \frac{1}{e^{-\lambda^2}}\)


## Exercise 2.2: Finding the median 
by definition, the mean is the value s.t the F(x) = 0.5
\[F_X(x) = 0.5\]
\[1 - e^{- \lambda x} = 0.5\]
\[0.5  = e^{- \lambda x}\]
\[ln(0.5)  = - \lambda x\]
\[x = \frac{ln(0.5)}{- \lambda} \]

## Exercise 3: Finding Mean and Variance of \( Y \)

### Mean

The mean \( E(X) \) is given by:
\[
E(X) = \int_{0}^{\infty} x f(x) \, dx
\]
where \( f(x) = \lambda e^{-\lambda x} \). Calculate the integral:
\[
E(X) = \int_{0}^{\infty} x \lambda e^{-\lambda x} \, dx
\]
## Solution for \( E(X) = \int_{0}^{\infty} x \lambda e^{-\lambda x} \, dx \)

We want to evaluate the expected value of \( X \), which is given by the integral:

\[
E(X) = \int_{0}^{\infty} x \lambda e^{-\lambda x} \, dx
\]

We can use integration by parts. Let:
- \( u = x \) and \( dv = \lambda e^{-\lambda x} \, dx \).
  
Then, differentiate and integrate to get:
- \( du = dx \)
- \( v = -e^{-\lambda x} \)

Using the formula \( \int u \, dv = uv - \int v \, du \), we get:

\[
E(X) = \left[ -x e^{-\lambda x} \right]_{0}^{\infty} + \int_{0}^{\infty} e^{-\lambda x} \, dx
\]

1. At \( x = \infty \), \( -x e^{-\lambda x} \to 0 \).
2. At \( x = 0 \), \( -x e^{-\lambda x} = 0 \).

So, the boundary terms sum to zero, leaving:

\[
E(X) = \int_{0}^{\infty} e^{-\lambda x} \, dx
\]

Now we integrate:

\[
\int_{0}^{\infty} e^{-\lambda x} \, dx = \frac{1}{\lambda}
\]

Thus, the expected value is:

\[
E(X) = \frac{1}{\lambda}
\]

### Variance

The variance \( \text{Var}(X) \) is given by:
\[
\text{Var}(X) = E(X^2) - (E(X))^2
\]

Using \( E(X) = \frac{1}{\lambda} \), we calculate \( E(X^2) \) as:
\[
  \text{Var}(X) = \frac{2}{\lambda^2} - \frac{1}{\lambda^2} 
\]
Therefore, we get:
\[
\text{Var}(X)= \frac{1}{\lambda^2}
\]

## Exercise 5: Properties of Expected Values and Variance

Having two random variables \( X \) and \( Y \) (not necessarily independent), we know:

\( E(X + Y) = E(X) + E(Y) \)
\[
\text{Var}(X)= \sigma_x^2
\]
\[
\text{Var}(Y)= \sigma_y^2
\]
Using the result 
\[
\text{Var}(X + Y)= \sigma_x^2 + \sigma_y^2 + 2\sigma_{xy}^2
\]
\[
\text{Var}(X - Y)= \sigma_x^2 + \sigma_y^2 - 2\sigma_{xy}^2
\]
!!! not sure

\frac{1}{\lambda^2}
2. If \( X \sim N(\mu_X, \sigma_X^2) \) and \( Y \sim N(\mu_Y, \sigma_Y^2) \), then:
   \[
   \text{Var}(X + Y) = \sigma_X^2 + \sigma_Y^2 + 2 \, \text{Cov}(X, Y)
   \]

For independent \( X \) and \( Y \):
\[
\text{Var}(X + Y) = \sigma_X^2 + \sigma_Y^2
\]



### CS: Chapter 3

 exercises 3.5
```{r}
# Ax = y
set.seed(0)
n <- 1000
A <- matrix(runif(n * n), n, n)
x.true <- runif(n)
y <- A %*% x.true

# b)

start_time <- Sys.time()
A_inv <- solve(A)
x1 <- A_inv %*% y
end_time <- Sys.time()
time_taken <- end_time - start_time

# Calculate the mean absolute error
mean_absolute_error <- mean(abs(x1 - x.true))

print(time_taken)
print(mean_absolute_error)

# c)
start_time_direct <- Sys.time()
x2 <- solve(A, y)
end_time_direct <- Sys.time()
time_taken_direct <- end_time_direct - start_time_direct

# Calculate the mean absolute error
mean_absolute_error_direct <- mean(abs(x2 - x.true))

print(time_taken_direct)
print(mean_absolute_error_direct)

# d) Conclusion:

# Calculate time to explicitly form the inverse matrix A^-1 and solve Ax = y
start_time <- Sys.time()
A_inv <- solve(A)
x1 <- A_inv %*% y
end_time <- Sys.time()
time_taken <- end_time - start_time

# Calculate the mean absolute error between x1 and x.true
mean_absolute_error <- mean(abs(x1 - x.true))

print(time_taken)
print(mean_absolute_error)

# Calculate time to directly solve Ax = y without forming A^-1
start_time_direct <- Sys.time()
x2 <- solve(A, y)
end_time_direct <- Sys.time()
time_taken_direct <- end_time_direct - start_time_direct

# Calculate the mean absolute error between x2 and x.true
mean_absolute_error_direct <- mean(abs(x2 - x.true))

print(time_taken_direct)
print(mean_absolute_error_direct)

```
Conclusion:
Using 'solve' to directly solve the equation Ax = y is significantly faster (approximately 0.39 seconds) compared to explicitly forming A^-1 and then multiplying it by y (approximately 2.29 seconds).

While both solutions are very precise, directly solving with 'solve' yields a slightly smaller mean absolute error (approximately 1.36e-12) compared to forming the inverse (approximately 2.96e-11).

Therefore, solving the linear system directly without calculating the explicit inverse is preferable 
in terms of both efficiency and accuracy.


es 3.6

```{r}
# a)
# Function to calculate the ECDF
ecdf_values <- function(x) {
  # Total number of observations
  n <- length(x)
  
  # Sort x values with sort.int, keeping the original indices
  sorted_x <- sort.int(x, index.return = TRUE)
  
  # Calculate the ECDF for each value in x
  ecdf <- (1:n) / n
  
  # Reorder the ECDF values to the original order
  ecdf_original_order <- ecdf[order(sorted_x$ix)]
  
  return(ecdf_original_order)
}

# Test the function with an example
set.seed(123)
x <- rnorm(10)
ecdf_values(x)


# b)
# Function to calculate the ECDF and optionally plot it
ecdf_values <- function(x, plot.cdf = FALSE) {
  # Total number of observations
  n <- length(x)
  
  # Sort x values
  sorted_x <- sort(x)
  
  # Calculate the ECDF for each value in x
  ecdf <- sapply(x, function(xi) sum(sorted_x < xi) / n)
  
  # If plot.cdf is TRUE, plot the ECDF
  if (plot.cdf) {
    plot(sort(x), ecdf[order(x)], type = "s", main = "Empirical Cumulative Distribution Function",
         xlab = "x", ylab = "ECDF")
  }
  
  return(ecdf)
}

# Test the function with an example
set.seed(123)
x <- rnorm(10)
ecdf_values(x, plot.cdf = TRUE)

# Generate random samples
set.seed(123)
x_norm <- rnorm(100)
x_unif <- runif(100)

# Test the function with normal distribution
ecdf_norm <- ecdf_values(x_norm, plot.cdf = TRUE)

# Test the function with uniform distribution
ecdf_unif <- ecdf_values(x_unif, plot.cdf = TRUE)
```



### FSDS: Chapter 2, exercises 2.8, 2.16, 2.21, 2.26, 2.52, 2.53, 2.70



### FSDS: Chapter 3, exercises 3.18, 3.28, 3.24 


```{r exercise_b3}
# Placeholder for code for Exercise B3
```

### FSDS: Chapter 4, exercises 4.14, 4.16, 4.48 

# Exercise 4.14
```{r 4.14}
data = read.table("https://stat4ds.rwth-aachen.de/data/Students.dat", header = TRUE)
summary(data)
#a
x_bar = mean(data$tv)
s = sd(data$tv)
# H0: the mean is 7.2
# H1: the mean is different from 7.2
z = qnorm(0.975) # having n > 60, we can assume normality
SE = s/sqrt(length(data$tv))
CI = x_bar + c(-1, 1)*z*SE
# we can say that 95% of the students on average spend between 5.30 and almost 9 hours watching TV per week

#b
only_male = data %>% filter(gender == 0)
only_female = data %>% filter(gender == 1)

boxplot(only_male$tv, only_female$tv, names = c("Males", "Females"), ylab = "weekly hours watching tv")
```
assuming
-independent populations
-equal variances
-normality -> weakest assumption, barely 30 observations
--> use t student's distribution
```{r}
x_bar_male = mean(only_male$tv)
x_bar_female = mean(only_female$tv)
s_male = sd(only_male$tv)
s_female = sd(only_female$tv)
n_male = length(only_male$tv)
n_female = length(only_female$tv)

# assuming equal variance
s_p = ((n_male -1)*(s_male^2) + (n_female - 1)*(s_female^2))/(n_male + n_female -1)
# t = (x_bar_male - x_bar_female)/(s_p*(1/n_male + 1/n_female))
SE_diff = (x_bar_male - x_bar_female)/((s_p^2)*(1/n_male + 1/n_female))
mean_diff = x_bar_male - x_bar_female
z = qt(0.975, n_male + n_female - 2)
CI_diff = mean_diff + c(-1, 1)*z*SE_diff

# Test for equality of means
t.test(only_male$tv, only_female$tv, conf.level = 0.05)
```

We can not state that the variances are significantly different betweent the 2 groups
However the confidence interval seems to suggest that females watch slightly more tv than males.

```{r}
# assuming unequal variance
SE_diff = sqrt((s_male^2)/n_male + (s_female^2)/n_female)
# t = (x_bar_male - x_bar_female)/(s_p*(1/n_male + 1/n_female))
mean_diff = x_bar_male - x_bar_female
z = qt(0.975, n_male + n_female - 2)
CI_diff_uvars = mean_diff + c(-1, 1)*z*SE_diff
```

Now we the interval crosses 0?? how is such a different result possible?

# Exercise 4.16
```{r 4.16}

data = read.table("https://stat4ds.rwth-aachen.de/data/Substance.dat", header = TRUE)

# compare alcohol users and non-users
alpha = 0.05
n = sum(data$count)
# find the total number of students that have or haven't used alcohol
N_alcohol_total = sum(data[data$alcohol == "yes", 4])
N_NOalcohol_total = sum(data[data$alcohol == "no", 4])
N_marijuana = sum(data[data$alcohol == "yes" & data$marijuana == "yes", 4])
N_NOalcohol_marijuana = sum(data[data$alcohol == "no" & data$marijuana == "yes", 4])

pi_1_hat = N_marijuana/N_alcohol_total
pi_2_hat = N_NOalcohol_marijuana/N_NOalcohol_total

# by hand
z = qnorm(1-alpha/2)
SE = (pi_1_hat*(1 - pi_1_hat)/N_alcohol_total + pi_2_hat*(1 - pi_2_hat)/N_NOalcohol_total)
CI_prop = (pi_1_hat - pi_2_hat) + c(-1, 1)*z*sqrt(SE)

#with software
prop.test(c(955, 5), c(1949, 327), conf.level = alpha, correct = F)
```
Interpretation: there seems to be a significant mean difference in the use of marijuana 
between students that used alcohol before and students who didn't.


# Exercise 4.48

## Given

\[
\text{Given } \quad SE = \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}
\]

With 95% likelihood, find:
$$ 
SE \leq \frac{1}{\sqrt{n}} \quad \text{when } \hat{p} = 0.5 
$$
This is where \( SE \) is maximized.

\[
SE = \sqrt{\frac{0.5 \times (0.5)}{n}} = \frac{\sqrt{0.25}}{\sqrt{n}} = \frac{0.5}{\sqrt{n}} = \frac{1}{2 \sqrt{n}}
\]

## Example: When $\hat{p} \neq 0.5$

\[
SE = \sqrt{\frac{0.3 \times (1 - 0.3)}{n}} = \sqrt{\frac{0.21}{n}} = \frac{0.458}{\sqrt{n}} \approx \frac{0.91}{2\sqrt{n}}
\]

## Example: When $\hat{p} = 0.2$

\[
SE = \sqrt{\frac{0.2 \times (0.8)}{n}} = \sqrt{\frac{0.16}{n}} = \frac{0.4}{\sqrt{n}} = \frac{0.8}{2\sqrt{n}}
\]

## Example: When $\hat{p} = 0.7$

\[
SE = \sqrt{\frac{0.7 \times (0.3)}{n}} = \sqrt{\frac{0.21}{n}} = \frac{0.458}{\sqrt{n}} \approx \frac{0.91}{2\sqrt{n}}
\]

## Observation

\[
\text{Note that when both } \hat{p} < 0.5 \text{ and } \hat{p} > 0.5 \text{ the numerator decreases, along with the standard error.}
\]

## For Maximum Standard Error

Set the maximum \( SE \) to be within margin \( M \).

\[
\frac{1}{\sqrt{n}} = M
\]

\[
\Rightarrow \frac{1}{M} = \sqrt{n} \Rightarrow n = \frac{1}{M^2}
\]

As long as \( n \geq \frac{1}{M^2} \), our error will be within \( M \).



### FSDS: Chapter 5, exercises 5.2, 5.12, 5.50 ###

```{r exercise_b5}
# Placeholder for code for Exercise B5
```

FSDS: chapter 5, exercise 5.2, 5.42, 5.50

## Esercizio 5.2

### Dati e Definizione del Problema

Nel problema viene fornito che:
- The total population is \( N = 200 \)
- The sample size is \( n = 624 \)
- The observed sample proportion is \( \hat{p} = 0.52 \) (ovvero 52%)

### Formulazione delle Ipotesi

Poniamo le seguenti ipotesi per il test statistico:
- **Ipotesi nulla \( H_0 \)**: \( \pi = 0.50 \)
- **Ipotesi alternativa \( H_1 \)**: \( \pi \neq 0.50 \)

### Calcolo del Test Statistico

We use a z-test to compare the sample proportion with the hypothesized proportion. We then calculate the z-value and the p-value.

```{r}
# Dati
N <- 200
n <- 624
p_hat <- 0.52  # Percentuale 52%

# Calcolo del valore di z
pi_0 <- 0.50  # valore ipotizzato sotto H0
z <- (p_hat - pi_0) / sqrt((pi_0 * (1 - pi_0)) / N)
z  # Mostra il valore di z

# Calcolo del p-value
p_value <- 2 * (1 - pnorm(abs(z)))
p_value  # Output del p-value

**Given that the p-value is 0.2646 and greater than 0.05, we cannot reject the null hypothesis**

## Menu A

$$
\bar{X}_a = 22.30
$$

$$
\sigma_a = 6.88
$$

$$
n_a = 43
$$

## Menu B

$$
\bar{X}_b = 25.91
$$

$$
\sigma_b = 8.01
$$

$$
n_b = 50
$$

## Hypotheses

$$
H_0 : \mu_a = \mu_b \quad (\text{le medie sono uguali}) 
$$

$$
H_a : \mu_a \neq \mu_b
$$

## t-test

$$
t = \frac{(\bar{X}_a - \bar{X}_b)}{\sqrt{\frac{\sigma_a^2}{n_a} + \frac{\sigma_b^2}{n_b}}} = \frac{22.30 - 25.91}{\sqrt{\frac{6.88^2}{43} + \frac{8.06^2}{50}}} \approx -2.34
$$

## Results

$$
df \approx 89
$$

$$
SE = 1.544
$$

$$
p\text{-value} = 0.021
$$


**ES 5.50**

Given that the p-value is 0.057, and greater than 0.05, we cannot reject the null hypothesis.

1. The p-value of 0.057 means there is a 5.7% probability of obtaining a result like the one observed (or more extreme) if the null hypothesis is true.
2. The p-value of 0.057 indicates the probability of obtaining a test value as extreme as 120 or more extreme, given that the null hypothesis is true.
3. The p-value of 0.057 does not represent the probability of a Type I error. The probability of a Type I error is determined by the significance level \(\alpha\), not by the p-value.
4. We cannot accept $H_0$; we can only fail to reject it. Since the p-value of 0.057 is greater than 0.05, we do not have sufficient evidence to reject the null hypothesis at the 5% significance level.
!!! da correggere

